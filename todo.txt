

=== general stuff ===

 * add some kind of debug flag to the makefile, which will enable/disable
   FLINT_ASSERT
   
 * we should be checking somewhere (at build time?) that FLINT_BITS_PER_LIMB
   really is the same as GMP's bits per limb
   
 * add some documentation for cache hint macros in flint.h
 
 * determine cache size at build time, instead of #defined in flint.h!!!!
     * possibly determine size of each cache in hierarchy?
 
 * generally clean up the makefile -- it's really grossly disgusting right now

 * Write some decent profiling code.

 * Make sure to use mpz_rrandomb for all randomised testing, it's much better
   at picking up corner cases

 * Think more about the odd-even karatsuba idea -- david suspects it might
   give smoother performance for unequal length inputs

 * Finish implementations of all polynomial multiplication routines and
integrate them into a single routine

 * Profile our code to date in the Z and polynomials over Z packages.

 * Write a test module for Z and polynomials over Z packages. Move all
 existing test code into that module.

 * Change Zvec to Zpoly throughout and implement Zvec routines. (Bill)

 * Collect together timing statistics on various programs (MAGMA, NTL, PARI,
FLINT, etc), in one convenient place. (David)

 * Change MAGMA timing routines to generate new random polys say every 20
iterations (david)

 * Document the Z package that has already been written (Bill).

 * Write an asymptotically fast GCD algorithm for the Z package for very
large integers. (First check how far along the GMP implementation is, and
compare it speedwise to MAGMA (which presumably is very fast :-) )

 * line endings. All of bill's files have DOS/Windows line endings :-) Some of
the other files don't. Is this a problem?

 * get rid of "(check%5==0)" test in mpn_extras -- unnecessary division here
 * follow up issue related to arithmetic right shifts --- see NTL's #define for this. Add test code to the build process to check for this.

 * write threadsafe limb allocator (does this even make sense at all with a
   stack-based model?)


=== Build process ===

 * Figure out `FLINT_BITS_PER_LIMB`, put it in a header somewhere (do ''not''
just use `gmp_bits_per_limb` because that's a const, not a #define)

 * Figure out FLINT_CACHE_SIZE


=== ssfft module ===

* continue working on ssfft3 branch

* GMP has a cool idea for doing very long butterflies. Instead of doing
a call to mpn_add_n and mpn_sub_n separately, it works in chunks to ensure
everything is done in cache. This will only make a difference when the
coefficients are so large they don't even fit in L1 any more. To do this
*properly* it would need to work for rotations as well.

* Is there a way to do reduce_mod_p_exact with one pass over the data in the
worst case? (Currently the worst case is two passes.)

* study the the overflow bit guarantees more carefully in the
  inverse transform. The cross butterflies seem to add a factor of 3 rather
  than 2 to the errors. Currently we are being too conservative in doing
  fast reductions; we could get away with fewer, but would need to do a
  safety reduction pass over the data after a certain number of layers.


=== ssmul module ===

* maybe it's not optimal to store the coefficients n+1 limbs apart. Bill
mentioned some potential cache-thrashing issues on intels. Might be better
to add a bit of padding.

* the 0th and (n/2)th fourier coefficients are always about half the size of
  the others. (These are the two stored first in the bit-reversed array.)
  Take advantage of this when doing pointwise mults there.

* negacylic convolutions for pointwise mults when the coefficients are
  large enough. Also chase up that reference in the paper by Zimmerman et al
  for some kind of wrapped-around toom cook.

* for KS, try the idea where we evaluate X1 = f(2^n)g(2^n), X2 = f(-2^n)g(-2^n) and then reconstruct output from X1+X2 and X1-X2



=== Zpoly_mpn module ===

* clarify the meaning of the sign limb. The documentation in the header
  doesn't agree with what the code seems to do.



=== modpmul module ===

* try writing assembly version of mul_redc. Maybe the three multiplications
can be pipelined. Also, it might be possible to write a version which does
two independent multiplications in parallel, and gets some pipelining
happening that way. But that would require rewriting FFTs to take advantage
of it.

* consider writing a radix-4 or split-radix FFT. I don't understand these too
well, but it seems like these only give a speedup on "complex" data. There
was a paper Bill mentioned that simulates a "complex" FFT by working over
GF(p^2), so perhaps this could be used.

* see whether doing two FFT's at once saves time on computing roots of unity
and whether the multiplications being data independent might allow them to be
interleaved and thus overlapped due to pipelining on the Opteron.

* try to speed up basecase matrix transposition code

* examine NTL's modmuls more carefully. Benchmark just the modmuls by
themselves.

* think about the ordering of loops in each stage of the outer fft routine.
Sometimes might be slightly better to start at the end and work backwards
to improve locality.
